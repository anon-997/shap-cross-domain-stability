# -*- coding: utf-8 -*-
"""SHAP consistency sentiment analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18AwJUUsGpj9m0kZRIHUsG3hY3Yw6lyd9

# 1. Introduction and Experimental Motivation

This notebook investigates the stability of SHAP explanations for sentiment analysis models under cross-domain shift.
While prior work has shown that predictive performance and fairness metrics can degrade across domains, less is known about whether post-hoc explanations remain consistent and reliable under such shifts.

We focus on three domains with substantially different linguistic properties:
- Twitter (short, informal, noisy text)
- Movie reviews (long, narrative text)
- Product reviews (medium-length, evaluative text)

The goal of this study is to quantify explanation stability, not to improve raw predictive performance.

# 2. Environment Setup and Reproducibility

This section installs required libraries and fixes random seeds to ensure reproducibility.
"""

!pip -q install -U datasets transformers accelerate shap evaluate scipy

!pip uninstall -y pyarrow pandas datasets evaluate transformers

!pip install -U \
  pyarrow==14.0.2 \
  pandas==2.1.4 \
  datasets==2.16.1 \
  transformers==4.36.2 \
  evaluate==0.4.1

import os, random, numpy as np, pandas as pd

from datasets import load_dataset
from transformers import pipeline
import evaluate
import shap
import numpy as np

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

"""# 3. Dataset Acquisition

We load three publicly available English sentiment datasets from Hugging Face:
- TweetEval (sentiment subset)
- IMDB movie reviews
- Amazon Polarity product reviews

"""

# TweetEval sentiment: text field is "text", label field is "label"
ds_twitter = load_dataset("cardiffnlp/tweet_eval", "sentiment")  # :contentReference[oaicite:4]{index=4}

# IMDB: text field is "text", label field is "label"
ds_imdb = load_dataset("stanfordnlp/imdb")  # :contentReference[oaicite:5]{index=5}

# Amazon Polarity: text field is "text", label field is "label"
ds_amazon = load_dataset("SetFit/amazon_polarity")  # :contentReference[oaicite:6]{index=6}

"""# 4. Dataset Harmonization and Sampling

To enable cross-domain comparison, all datasets are mapped to a binary sentiment label space (negative vs. positive).
Neutral samples are removed where applicable.

Each dataset is balanced by sampling an equal number of positive and negative examples.

"""

def sample_tweeteval_binary(ds, n_per_class=300, seed=42):
    # TweetEval sentiment labels: 0=negative, 1=neutral, 2=positive
    df = ds["test"].to_pandas()
    df = df[df["label"].isin([0, 2])]

    neg = df[df["label"] == 0].sample(n_per_class, random_state=seed)
    pos = df[df["label"] == 2].sample(n_per_class, random_state=seed)

    out = pd.concat([neg, pos], ignore_index=True).sample(frac=1, random_state=seed).reset_index(drop=True)

    # map {0,2} -> {0,1}
    out["label"] = out["label"].map({0: 0, 2: 1})
    return out["text"].tolist(), out["label"].tolist()


def sample_binary(ds, split="test", n_per_class=300, seed=42):
    df = ds[split].to_pandas()

    neg = df[df["label"] == 0].sample(n_per_class, random_state=seed)
    pos = df[df["label"] == 1].sample(n_per_class, random_state=seed)

    out = pd.concat([neg, pos], ignore_index=True).sample(frac=1, random_state=seed).reset_index(drop=True)
    return out["text"].tolist(), out["label"].tolist()

X_tw, y_tw = sample_tweeteval_binary(ds_twitter, n_per_class=300, seed=SEED)
X_im, y_im = sample_binary(ds_imdb, "test", n_per_class=300, seed=SEED)
X_am, y_am = sample_binary(ds_amazon, "test", n_per_class=300, seed=SEED)

len(X_tw), len(X_im), len(X_am)

"""# 5. Model Selection and Inference Pipeline

We use a single fixed sentiment classifier across all domains:
`distilbert-base-uncased-finetuned-sst-2-english`.

This isolates the effect of domain shift on explanations, rather than model architecture or training differences.

"""

from transformers import pipeline

clf = pipeline(
    "sentiment-analysis",
    model="distilbert/distilbert-base-uncased-finetuned-sst-2-english",
    return_all_scores=True
)

def predict_proba(texts):
    outs = clf(texts, truncation=True)
    probs = []
    for o in outs:
        d = {x["label"].upper(): x["score"] for x in o}
        probs.append([d.get("NEGATIVE", 0.0), d.get("POSITIVE", 0.0)])
    return np.array(probs)

"""# 6. Sanity Checks and Baseline Behavior

Before conducting explanation analysis, we verify that the model produces non-degenerate predictions across domains.
These checks ensure the inference pipeline is functioning correctly.

"""

def quick_check(X, y, name):
    probs = predict_proba(X[:20])
    preds = probs.argmax(axis=1)
    print(name)
    print("Preds :", preds[:10])
    print("Labels:", y[:10])
    print()

quick_check(X_tw, y_tw, "Twitter")
quick_check(X_im, y_im, "IMDB")
quick_check(X_am, y_am, "Amazon")

"""# 7. Dataset Characterization and Domain Differences

We characterize each dataset in terms of:
- Dataset size and label balance
- Text length distributions
- Representative examples
- Vocabulary overlap across domains

These analyses contextualize expected explanation instability.

"""

def dataset_summary(X, y, name):
    print(f"\n{name} dataset summary")
    print("-" * 40)
    print("Number of samples:", len(X))
    print("Label distribution:")
    print(pd.Series(y).value_counts().sort_index())

dataset_summary(X_tw, y_tw, "Twitter (TweetEval)")
dataset_summary(X_im, y_im, "IMDB")
dataset_summary(X_am, y_am, "Amazon Polarity")

def text_length_stats(X, name):
    lengths = [len(x.split()) for x in X]
    s = pd.Series(lengths)
    print(f"\n{name} text length stats (words)")
    print(s.describe())

text_length_stats(X_tw, "Twitter")
text_length_stats(X_im, "IMDB")
text_length_stats(X_am, "Amazon")

def show_examples(X, y, name, n=3):
    print(f"\n{name} examples")
    print("-" * 40)
    for i in range(n):
        label = "POS" if y[i] == 1 else "NEG"
        print(f"[{label}] {X[i][:300]}")
        print()

show_examples(X_tw, y_tw, "Twitter")
show_examples(X_im, y_im, "IMDB")
show_examples(X_am, y_am, "Amazon")

def vocab_set(X):
    vocab = set()
    for text in X:
        vocab.update(text.lower().split())
    return vocab

v_tw = vocab_set(X_tw)
v_im = vocab_set(X_im)
v_am = vocab_set(X_am)

def jaccard(a, b):
    return len(a & b) / len(a | b)

print("Vocab Jaccard Twitter–IMDB:", jaccard(v_tw, v_im))
print("Vocab Jaccard Twitter–Amazon:", jaccard(v_tw, v_am))
print("Vocab Jaccard IMDB–Amazon:", jaccard(v_im, v_am))

"""# 8. Explainability Method: SHAP

We use SHAP to explain the positive-class probability produced by the sentiment classifier.
All explanations are generated using a tokenizer-aware text masker to ensure alignment with model inputs.

"""

import shap
from collections import defaultdict

# ---- batched model for GLOBAL SHAP ----
def shap_model_batch(texts):
    texts = [str(t) for t in texts]
    probs = predict_proba(texts)      # shape: (N, 2)
    return probs[:, 1]                # POSITIVE class, shape: (N,)

# ---- tokenizer-aware masker ----
masker = shap.maskers.Text(clf.tokenizer)

data_to_save = {}

if "global_tw" in globals():
    data_to_save["twitter"] = global_tw
if "global_im" in globals():
    data_to_save["imdb"] = global_im
if "global_am" in globals():
    data_to_save["amazon"] = global_am

import pickle

with open("/content/global_shap_outputs_final.pkl", "rb") as f:
    shap_data = pickle.load(f)

global_tw = shap_data["twitter"]
global_im = shap_data["imdb"]
global_am = shap_data["amazon"]

with open("/content/global_shap_twitter_only.pkl", "rb") as f:
    global_tw = pickle.load(f)["twitter"]

# Shared vocabulary across all domains
V_shared = set(global_tw.keys()) & set(global_im.keys()) & set(global_am.keys())
len(V_shared)

import pickle

assert global_tw is not None
print("Twitter SHAP loaded. Token count:", len(global_tw))

import torch

print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

import numpy as np
import shap
from collections import defaultdict

# --- DistilBERT max length ---
MAX_TOKENS = getattr(clf.tokenizer, "model_max_length", 512)
if MAX_TOKENS is None or MAX_TOKENS > 10_000:
    MAX_TOKENS = 512

def truncate_for_model(text: str, max_tokens: int = MAX_TOKENS) -> str:
    text = "" if text is None else str(text)
    ids = clf.tokenizer.encode(text, truncation=True, max_length=max_tokens)
    return clf.tokenizer.decode(ids, skip_special_tokens=True)

def prep_texts(texts, max_tokens: int = MAX_TOKENS):
    out = []
    for t in texts:
        if t is None:
            continue
        s = str(t).strip()
        if not s:
            continue
        out.append(truncate_for_model(s, max_tokens))
    return out

def predict_proba(texts):
    texts = [str(t) for t in texts]
    outs = clf(texts, truncation=True)
    probs = []
    for o in outs:
        d = {x["label"].upper(): x["score"] for x in o}
        probs.append([d.get("NEGATIVE", 0.0), d.get("POSITIVE", 0.0)])
    return np.array(probs)

def shap_model_batch(texts):
    texts = prep_texts(texts)
    probs = predict_proba(texts)
    return probs[:, 1]  # POSITIVE class

def extract_token_importance(shap_values):
    tokens = shap_values.data
    vals = np.array(shap_values.values)

    if vals.ndim == 2 and vals.shape[1] == 1:
        vals = vals[:, 0]

    vals = np.abs(vals)

    pairs = []
    for t, v in zip(tokens, vals):
        if isinstance(t, str) and t.strip():
            pairs.append((t.lower(), float(v)))
    return pairs

masker = shap.maskers.Text(clf.tokenizer)

global_explainer = shap.Explainer(
    shap_model_batch,
    masker,
    algorithm="partition"
)

def global_shap_scores(texts, max_samples=20):
    token_scores = defaultdict(list)

    texts_small = prep_texts(texts[:max_samples])
    shap_values = global_explainer(texts_small)

    for sv in shap_values:
        for tok, val in extract_token_importance(sv):
            token_scores[tok].append(val)

    return {
        tok: float(np.mean(vals))
        for tok, vals in token_scores.items()
    }

import pickle, time
from collections import defaultdict

SAVE_PATH = "/content/global_shap_autosave_long.pkl"

# ---- sanity check ----
assert global_tw is not None, "Twitter SHAP must already be loaded"

# ---- load previous progress if exists ----
try:
    with open(SAVE_PATH, "rb") as f:
        shap_results = pickle.load(f)
    print("Loaded existing autosave:", shap_results.keys())
except FileNotFoundError:
    shap_results = {"twitter": global_tw}
    print("Starting fresh autosave.")

def autosave():
    with open(SAVE_PATH, "wb") as f:
        pickle.dump(shap_results, f)
    print(f"Autosaved → {SAVE_PATH}")

autosave()

# ---- CONFIG ----
CHUNK_SIZE = 5          # SHAP batch per iteration (safe)
MAX_IMDB = 120          # realistic upper bound in long run
MAX_AMAZON = 120

# ---- helper: accumulate SHAP incrementally ----
def accumulate_global_shap(texts, max_total, existing=None, label="dataset"):
    token_scores = defaultdict(list)

    if existing:
        for tok, val in existing.items():
            token_scores[tok].append(val)

    processed = 0
    start_time = time.time()

    while processed < max_total:
        batch = texts[processed:processed + CHUNK_SIZE]
        if not batch:
            break

        print(f"{label}: processing samples {processed} → {processed + len(batch)}")

        shap_values = global_explainer(prep_texts(batch))

        for sv in shap_values:
            for tok, val in extract_token_importance(sv):
                token_scores[tok].append(val)

        processed += len(batch)

        # periodic autosave
        autosave()

        # small sleep to keep Colab stable
        time.sleep(1)

    return {tok: float(sum(v) / len(v)) for tok, v in token_scores.items()}

# ---- IMDB ----
print("\n Starting IMDB long-run SHAP")
existing_imdb = shap_results.get("imdb")

global_im = accumulate_global_shap(
    X_im,
    max_total=MAX_IMDB,
    existing=existing_imdb,
    label="IMDB"
)

shap_results["imdb"] = global_im
autosave()
print("IMDB done. Tokens:", len(global_im))

# ---- AMAZON ----
print("\n Starting Amazon long-run SHAP")
existing_amazon = shap_results.get("amazon")

global_am = accumulate_global_shap(
    X_am,
    max_total=MAX_AMAZON,
    existing=existing_amazon,
    label="Amazon"
)

shap_results["amazon"] = global_am
autosave()
print("Amazon done. Tokens:", len(global_am))

# ---- FINAL SAVE ----
FINAL_PATH = "/content/global_shap_outputs_final.pkl"
with open(FINAL_PATH, "wb") as f:
    pickle.dump(shap_results, f)

print("\n LONG RUN COMPLETE")
print("Saved final file:", FINAL_PATH)
print("Keys:", shap_results.keys())

import pickle

with open("/content/global_shap_outputs_final.pkl", "rb") as f:
    shap_data = pickle.load(f)

print(
    "Twitter tokens:", len(shap_data["twitter"]),
    "IMDB tokens:", len(shap_data["imdb"]),
    "Amazon tokens:", len(shap_data["amazon"])
)

from google.colab import files
files.download("/content/global_shap_outputs_final.pkl")

# # 8. Explainability Method: SHAP (Global)

# """
# We compute global SHAP token attributions by aggregating per-token absolute
# SHAP values across multiple instances. This provides a domain-level view
# of which lexical features drive model predictions.
# """

# import shap
# from collections import defaultdict

# # ---- batched model for GLOBAL SHAP ----
# def shap_model_batch(texts):
#     texts = [str(t) for t in texts]
#     probs = predict_proba(texts)      # shape: (N, 2)
#     return probs[:, 1]                # POSITIVE class, shape: (N,)

# # ---- tokenizer-aware masker ----
# masker = shap.maskers.Text(clf.tokenizer)

# # ---- GLOBAL explainer ----
# global_explainer = shap.Explainer(
#     shap_model_batch,
#     masker,
#     algorithm="partition"
# )

# def global_shap_scores(texts, max_samples=100):
#     token_scores = defaultdict(list)

#     shap_values = global_explainer(texts[:max_samples])

#     for sv in shap_values:
#         for tok, val in extract_token_importance(sv):
#             token_scores[tok].append(val)

#     return {
#         tok: float(np.mean(vals))
#         for tok, vals in token_scores.items()
#     }

MAX_GLOBAL = 100

global_tw = global_shap_scores(X_tw, MAX_GLOBAL)
global_im = global_shap_scores(X_im, MAX_GLOBAL)
global_am = global_shap_scores(X_am, MAX_GLOBAL)

MAX_GLOBAL = 100

global_tw = global_shap_scores(X_tw, MAX_GLOBAL)
global_im = global_shap_scores(X_im, MAX_GLOBAL)
global_am = global_shap_scores(X_am, MAX_GLOBAL)

import pickle

data_to_save = {}

if "global_tw" in globals():
    data_to_save["twitter"] = global_tw
if "global_im" in globals():
    data_to_save["imdb"] = global_im
if "global_am" in globals():
    data_to_save["amazon"] = global_am

with open("global_shap_outputs_partial.pkl", "wb") as f:
    pickle.dump(data_to_save, f)

from google.colab import files
files.download("global_shap_outputs_partial.pkl")

type(shap_data), shap_data.keys()

def safe_len(x):
    return len(x) if x is not None else None

safe_len(global_tw), safe_len(global_im), safe_len(global_am)

import pickle

with open("/content/global_shap_outputs_partial.pkl", "rb") as f:
    shap_data = pickle.load(f)

global_tw = shap_data.get("twitter")
global_im = shap_data.get("imdb")
global_am = shap_data.get("amazon")

len(global_tw), len(global_im), len(global_am)

import pickle

with open("/content/global_shap_twitter_only.pkl", "wb") as f:
    pickle.dump({"twitter": global_tw}, f)

from google.colab import files
files.download("global_shap_twitter_only.pkl")

shap_data.keys() == dict_keys(['twitter'])

# =========================
# GLOBAL SHAP (IMDB + Amazon) on GPU, small limits + safe truncation
# =========================

import os, pickle, numpy as np
import shap
from collections import defaultdict
import torch

# --- 0) Make sure GPU is actually available ---
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))

# --- 1) Force the HF pipeline model to GPU if possible ---
# (pipeline uses device index; -1 = CPU, 0 = first GPU)
try:
    clf.device  # just to see if it exists
except Exception:
    pass

if torch.cuda.is_available():
    try:
        clf.model.to("cuda")
        # pipeline sometimes tracks device internally; safest is recreate pipeline if needed
        # but moving model works in most cases.
        print("Moved model to CUDA.")
    except Exception as e:
        print("Could not move model to CUDA directly:", e)
        print("If this persists, recreate the pipeline with device=0.")


# --- 2) Helper: truncate texts to model max length (prevents 921>512 explosions) ---
# DistilBERT max length is typically 512 tokens.
MAX_TOKENS = getattr(clf.tokenizer, "model_max_length", 512)
if MAX_TOKENS is None or MAX_TOKENS > 10_000:
    MAX_TOKENS = 512  # fallback

def truncate_for_model(text: str, max_tokens: int = MAX_TOKENS) -> str:
    text = "" if text is None else str(text)
    ids = clf.tokenizer.encode(text, truncation=True, max_length=max_tokens)
    return clf.tokenizer.decode(ids, skip_special_tokens=True)

def prep_texts(texts, max_tokens: int = MAX_TOKENS):
    out = []
    for t in texts:
        if t is None:
            continue
        s = str(t).strip()
        if not s:
            continue
        out.append(truncate_for_model(s, max_tokens=max_tokens))
    return out


# --- 3) ---
# predict_proba(texts) should return np.array shape (N,2)
def predict_proba(texts):
    texts = [str(t) for t in texts]
    outs = clf(texts, truncation=True)  # pipeline will batch internally
    probs = []
    for o in outs:
        d = {x["label"].upper(): x["score"] for x in o}
        probs.append([d.get("NEGATIVE", 0.0), d.get("POSITIVE", 0.0)])
    return np.array(probs)


# --- 4) SHAP model wrapper (BATCH) for GLOBAL explanations ---
def shap_model_batch(texts):
    # SHAP passes an array-like of strings; keep it as list[str]
    texts = prep_texts(texts, max_tokens=MAX_TOKENS)
    probs = predict_proba(texts)     # (N,2)
    return probs[:, 1]              # (N,)

# --- 5) Token importance extraction (robust to SHAP output shapes) ---
def extract_token_importance(shap_values):
    tokens = shap_values.data
    vals = shap_values.values

    # SHAP can return (len_tokens,) or (len_tokens,1). Normalize to 1D.
    vals = np.array(vals)
    if vals.ndim == 2 and vals.shape[1] == 1:
        vals = vals[:, 0]
    vals = np.abs(vals)

    pairs = []
    for t, v in zip(tokens, vals):
        if isinstance(t, str) and t.strip():
            pairs.append((t.lower(), float(v)))
    return pairs

# --- 6) Build GLOBAL explainer ---
masker = shap.maskers.Text(clf.tokenizer)

global_explainer = shap.Explainer(
    shap_model_batch,
    masker,
    algorithm="partition"
)

# --- 7) GLOBAL SHAP aggregation (small max_samples recommended) ---
def global_shap_scores(texts, max_samples=20):
    token_scores = defaultdict(list)

    texts_small = prep_texts(texts[:max_samples], max_tokens=MAX_TOKENS)
    shap_values = global_explainer(texts_small)

    for sv in shap_values:
        for tok, val in extract_token_importance(sv):
            token_scores[tok].append(val)

    return {tok: float(np.mean(vs)) for tok, vs in token_scores.items()}

# --- 8) Run IMDB + Amazon with smaller limits (fast + stable) ---
MAX_GLOBAL_IMDB = 20
MAX_GLOBAL_AMAZON = 20

global_im = global_shap_scores(X_im, max_samples=MAX_GLOBAL_IMDB)
global_am = global_shap_scores(X_am, max_samples=MAX_GLOBAL_AMAZON)

print("Sizes:", len(global_im), len(global_am))


# --- 9) Save everything together (final) ---
# Make sure global_tw exists (loaded from pickle or computed earlier)
assert global_tw is not None, "global_tw is missing. Load your twitter-only pickle first."

with open("/content/global_shap_outputs_final.pkl", "wb") as f:
    pickle.dump(
        {
            "twitter": global_tw,
            "imdb": global_im,
            "amazon": global_am
        },
        f
    )

print("Saved:", "/content/global_shap_outputs_final.pkl")

"""# 9. Local SHAP Explainer"""

# =========================
# 9. Local SHAP Explainer
# =========================



# single-instance model wrapper
def shap_model_single(texts):
    texts = [str(t) for t in texts]
    probs = predict_proba(texts)   # (N,2)
    return probs[:, 1]             # positive class

masker = shap.maskers.Text(clf.tokenizer)

local_explainer = shap.Explainer(
    shap_model_single,
    masker,
    algorithm="partition"
)

def explain_one(text):
    text = str(text)
    sv = local_explainer([text])
    shap.plots.text(sv[0])

# Twitter examples
print("Twitter example")
explain_one(X_tw[0])

# IMDB examples (longer text, but truncated safely by tokenizer)
print("IMDB example")
explain_one(X_im[0])

# Amazon examples
print("Amazon example")
explain_one(X_am[0])

"""#10 — Global Explanation Stability Metrics




"""

import pickle

with open("/content/global_shap_outputs_final.pkl", "rb") as f:
    shap_data = pickle.load(f)

global_tw = shap_data["twitter"]
global_im = shap_data["imdb"]
global_am = shap_data["amazon"]

print(
    len(global_tw),
    len(global_im),
    len(global_am)
)

def top_k_tokens(global_shap, k=20):
    return [t for t, _ in sorted(global_shap.items(), key=lambda x: -x[1])[:k]]

top_tw = top_k_tokens(global_tw, k=20)
top_im = top_k_tokens(global_im, k=20)
top_am = top_k_tokens(global_am, k=20)

def jaccard(a, b):
    a, b = set(a), set(b)
    return len(a & b) / len(a | b)

print("Twitter–IMDB:", jaccard(top_tw, top_im))
print("Twitter–Amazon:", jaccard(top_tw, top_am))
print("IMDB–Amazon:", jaccard(top_im, top_am))

from scipy.stats import spearmanr

def spearman_overlap(g1, g2, k=50):
    common = list(set(g1) & set(g2))
    if len(common) < 5:
        return np.nan
    r1 = [g1[t] for t in common]
    r2 = [g2[t] for t in common]
    return spearmanr(r1, r2).correlation

print("Twitter–IMDB Spearman:", spearman_overlap(global_tw, global_im))
print("Twitter–Amazon Spearman:", spearman_overlap(global_tw, global_am))
print("IMDB–Amazon Spearman:", spearman_overlap(global_im, global_am))

# top 50

for k in [20, 50, 100]:
    print(f"\nTop-{k} Jaccard overlaps")
    print("Twitter–IMDB:", jaccard(top_k_tokens(global_tw, k), top_k_tokens(global_im, k)))
    print("Twitter–Amazon:", jaccard(top_k_tokens(global_tw, k), top_k_tokens(global_am, k)))
    print("IMDB–Amazon:", jaccard(top_k_tokens(global_im, k), top_k_tokens(global_am, k)))

import shap

# single-instance model (POS class)
def shap_model_single(text):
    probs = predict_proba([text])
    return probs[0, 1]

masker = shap.maskers.Text(clf.tokenizer)

local_explainer = shap.Explainer(
    shap_model_single,
    masker,
    algorithm="partition"
)

# choose 2 examples per domain
examples = {
    "Twitter": X_tw[:2],
    "IMDB": X_im[:2],
    "Amazon": X_am[:2]
}

for domain, texts in examples.items():
    print(f"\n{domain} local SHAP")
    for t in texts:
        sv = local_explainer(t)
        shap.plots.text(sv)

import shap
import numpy as np

# ---- batch-safe SHAP model (POSITIVE class) ----
def shap_model_local(texts):
    # SHAP always passes a list-like object
    texts = ["" if t is None else str(t) for t in texts]
    probs = predict_proba(texts)     # shape: (N, 2)
    return probs[:, 1]               # shape: (N,)

# ---- tokenizer-aware masker ----
masker = shap.maskers.Text(clf.tokenizer)

# ---- local explainer ----
local_explainer = shap.Explainer(
    shap_model_local,
    masker,
    algorithm="partition"
)

# ---- qualitative examples only (keep this small) ----
examples = {
    "Twitter": X_tw[:2],
    "IMDB": X_im[:2],
    "Amazon": X_am[:2]
}

# ---- run local SHAP ----
for domain, texts in examples.items():
    print(f"\n{domain} local SHAP")
    for i, t in enumerate(texts):
        print(f"{domain} example {i+1}")
        sv = local_explainer([str(t)])   # <-- NOTE: list[str], not str
        shap.plots.text(sv[0])

import pandas as pd

rows = []
for k in [20, 50, 100]:
    rows.append({
        "K": k,
        "Twitter–IMDB": jaccard(top_k_tokens(global_tw, k), top_k_tokens(global_im, k)),
        "Twitter–Amazon": jaccard(top_k_tokens(global_tw, k), top_k_tokens(global_am, k)),
        "IMDB–Amazon": jaccard(top_k_tokens(global_im, k), top_k_tokens(global_am, k)),
    })

df_jaccard = pd.DataFrame(rows)
df_jaccard

from scipy.stats import spearmanr

def spearman_overlap(d1, d2):
    shared = set(d1) & set(d2)
    if len(shared) < 5:
        return np.nan
    v1 = [d1[t] for t in shared]
    v2 = [d2[t] for t in shared]
    return spearmanr(v1, v2).correlation

print("Twitter–IMDB:", spearman_overlap(global_tw, global_im))
print("Twitter–Amazon:", spearman_overlap(global_tw, global_am))
print("IMDB–Amazon:", spearman_overlap(global_im, global_am))

import pandas as pd

df = pd.DataFrame({
    "Pair": ["Twitter–IMDB", "Twitter–Amazon", "IMDB–Amazon"],
    "Jaccard (Top-100)": [
        jaccard(top_k_tokens(global_tw, 100), top_k_tokens(global_im, 100)),
        jaccard(top_k_tokens(global_tw, 100), top_k_tokens(global_am, 100)),
        jaccard(top_k_tokens(global_im, 100), top_k_tokens(global_am, 100)),
    ],
    "Spearman": [
        0.1803,
        0.2586,
        0.1824
    ]
})

df

import pickle
from datetime import datetime

final_artifacts = {
    "global_shap": {
        "twitter": global_tw,
        "imdb": global_im,
        "amazon": global_am
    },
    "metadata": {
        "model": "distilbert-base-uncased-finetuned-sst-2-english",
        "date": datetime.now().isoformat(),
        "global_samples": {
            "twitter": len(global_tw),
            "imdb": len(global_im),
            "amazon": len(global_am)
        }
    }
}

with open("/content/shap_experiment_artifacts.pkl", "wb") as f:
    pickle.dump(final_artifacts, f)

print("Saved shap_experiment_artifacts.pkl")

from google.colab import files
files.download("/content/shap_experiment_artifacts.pkl")

import pickle

with open("/content/shap_experiment_artifacts.pkl", "rb") as f:
    artifacts = pickle.load(f)

global_tw = artifacts["global_shap"]["twitter"]
global_im = artifacts["global_shap"]["imdb"]
global_am = artifacts["global_shap"]["amazon"]

print("Reloaded SHAP:")
print(len(global_tw), len(global_im), len(global_am))

import matplotlib.pyplot as plt

def plot_top_tokens(global_scores, title, k=20):
    top = sorted(global_scores.items(), key=lambda x: x[1], reverse=True)[:k]
    tokens, values = zip(*top)

    plt.figure(figsize=(8, 4))
    plt.barh(tokens[::-1], values[::-1])
    plt.title(title)
    plt.xlabel("Mean |SHAP|")
    plt.tight_layout()
    plt.show()

plot_top_tokens(global_tw, "Twitter Top Tokens")
plot_top_tokens(global_im, "IMDB Top Tokens")
plot_top_tokens(global_am, "Amazon Top Tokens")

from scipy.stats import spearmanr
import numpy as np

def spearman_corr(global_a, global_b):
    # Align tokens present in both domains
    common_tokens = set(global_a.keys()) & set(global_b.keys())
    if len(common_tokens) == 0:
        return 0.0

    a_vals = [global_a[t] for t in common_tokens]
    b_vals = [global_b[t] for t in common_tokens]

    corr, _ = spearmanr(a_vals, b_vals)
    return float(corr) if not np.isnan(corr) else 0.0

import pandas as pd

stability_results = pd.DataFrame({
    "Pair": ["Twitter–IMDB", "Twitter–Amazon", "IMDB–Amazon"],
    "Spearman": [
        spearman_corr(global_tw, global_im),
        spearman_corr(global_tw, global_am),
        spearman_corr(global_im, global_am),
    ],
    "Jaccard@100": [
        jaccard(top_k_tokens(global_tw, 100), top_k_tokens(global_im, 100)),
        jaccard(top_k_tokens(global_tw, 100), top_k_tokens(global_am, 100)),
        jaccard(top_k_tokens(global_im, 100), top_k_tokens(global_am, 100)),
    ]
})

stability_results

stability_results

stability_results.to_csv("explanation_stability_results.csv", index=False)

import matplotlib.pyplot as plt
shap.plots.text(sv[0], show=False)
plt.savefig("twitter_local_shap_1.png", dpi=300, bbox_inches="tight")

"""## others

"""

import numpy as np
from scipy.stats import spearmanr

def bootstrap_spearman(v1, v2, n_boot=1000):
    idx = np.arange(len(v1))
    scores = []
    for _ in range(n_boot):
        sample = np.random.choice(idx, size=len(idx), replace=True)
        scores.append(spearmanr(v1[sample], v2[sample]).correlation)
    return np.mean(scores), np.percentile(scores, [5, 95])

mean_rho, ci = bootstrap_spearman(global_tw, global_im)
mean_rho, ci

def align_vectors(d1, d2):
    shared = sorted(set(d1.keys()) & set(d2.keys()))
    v1 = np.array([d1[t] for t in shared])
    v2 = np.array([d2[t] for t in shared])
    return v1, v2

v_tw_im = align_vectors(global_tw, global_im)
v_tw_am = align_vectors(global_tw, global_am)
v_im_am = align_vectors(global_im, global_am)

type(global_tw), type(global_im), type(global_am)



"""bootstraping"""

import pickle

with open("shap_experiment_artifacts.pkl", "rb") as f:
    artifacts = pickle.load(f)

type(artifacts)

artifacts.keys()

"""## per document shap"""

import shap
import torch
import numpy as np
import pickle
from transformers import AutoTokenizer, AutoModelForSequenceClassification

MODEL_NAME = "distilbert-base-uncased-finetuned-sst-2-english"

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
model.eval()

def predict_logits(texts):
    # SHAP may pass a single string or a list
    if isinstance(texts, str):
        texts = [texts]
    else:
        texts = list(texts)

    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )

    with torch.no_grad():
        outputs = model(**inputs)

    return outputs.logits.detach().cpu().numpy()

def predict_proba(texts):
    if isinstance(texts, str):
        texts = [texts]

    inputs = tokenizer(
        texts,
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors="pt"
    )

    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)

    return probs.numpy()

masker = shap.maskers.Text(
    tokenizer,
    mask_token=tokenizer.mask_token
)

explainer = shap.explainers.Partition(
    predict_proba,
    masker
)

sv = explainer(["test sentence"])
sv

import shap
print(shap.__version__)

def compute_per_doc_shap(texts, explainer, class_index=1, batch_size=8):
    texts = ensure_list_of_str(texts)
    per_doc_shap = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        shap_values = explainer(batch)

        for j in range(len(batch)):
            tokens = shap_values.data[j]
            values = shap_values.values[j][:, class_index]

            doc_dict = {}
            for tok, val in zip(tokens, values):
                if tok is None:
                    continue
                tok = tok.lower()
                doc_dict[tok] = doc_dict.get(tok, 0.0) + abs(float(val))

            per_doc_shap.append(doc_dict)

    return per_doc_shap

type(X_tw), type(X_tw[0])

for i in range(5):
    print(i, type(X_tw[i]), repr(X_tw[i])[:80])

def ensure_list_of_str(texts):
    return [str(t) for t in list(texts)]

X_tw = ensure_list_of_str(X_tw)
X_im = ensure_list_of_str(X_im)
X_am = ensure_list_of_str(X_am)

# X_tw, X_im, X_am are your 600 texts per domain

per_doc_tw = compute_per_doc_shap(X_tw, explainer)
per_doc_im = compute_per_doc_shap(X_im, explainer)
per_doc_am = compute_per_doc_shap(X_am, explainer)

len(per_doc_tw), len(per_doc_im), len(per_doc_am)

def predict_logits(texts):
    print("DEBUG type(texts):", type(texts))
    try:
        print("DEBUG texts example:", texts[:1])
    except Exception:
        print("DEBUG texts value:", texts)

    raise RuntimeError("STOP HERE")

_ = explainer(["test sentence"])

def integrated_gradients(text, target_class=1, steps=25):
    # Tokenize
    inputs = tokenizer(
        text,
        return_tensors="pt",
        truncation=True,
        max_length=512
    )

    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    # Get embeddings
    embeddings = model.get_input_embeddings()(input_ids)
    baseline = torch.zeros_like(embeddings)

    total_grads = torch.zeros_like(embeddings)

    # Integrated Gradients loop
    for alpha in torch.linspace(0, 1, steps):
        interp_emb = baseline + alpha * (embeddings - baseline)
        interp_emb.requires_grad_(True)

        outputs = model(
            inputs_embeds=interp_emb,
            attention_mask=attention_mask
        )

        logit = outputs.logits[0, target_class]

        grads = torch.autograd.grad(
            outputs=logit,
            inputs=interp_emb,
            retain_graph=False,
            create_graph=False
        )[0]

        total_grads += grads.detach()

    # Average gradients and compute IG
    avg_grads = total_grads / steps
    ig = (embeddings - baseline) * avg_grads

    # Aggregate per token
    token_scores = ig.norm(dim=-1).squeeze(0)
    tokens = tokenizer.convert_ids_to_tokens(input_ids.squeeze(0))

    return dict(zip(tokens, token_scores.cpu().numpy()))

def compute_per_doc_attributions(texts):
    per_doc = []
    for text in texts:
        attr = integrated_gradients(text)
        per_doc.append({k.lower(): abs(float(v)) for k, v in attr.items()})
    return per_doc

compute_per_doc_attributions(["test sentence"])

compute_per_doc_attributions(["test sentence"])